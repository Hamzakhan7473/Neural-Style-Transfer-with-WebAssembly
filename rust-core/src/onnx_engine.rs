use wasm_bindgen::prelude::*;
use wasm_bindgen_futures::JsFuture;
use web_sys::{Request, RequestInit, RequestMode, Response};
use js_sys::{ArrayBuffer, Uint8Array};
use serde::{Serialize, Deserialize};
use std::collections::HashMap;

#[derive(Serialize, Deserialize, Clone)]
pub struct ONNXModelMetadata {
    pub name: String,
    pub url: String,
    pub size_bytes: usize,
    pub input_shape: Vec<i64>,
    pub output_shape: Vec<i64>,
    pub input_tensor_name: String,
    pub output_tensor_name: String,
    pub recommended_resolution: (u32, u32),
    pub style_description: String,
}

#[derive(Serialize, Deserialize)]
pub struct StyleTransferResult {
    pub success: bool,
    pub output_data: Option<Vec<u8>>,
    pub error_message: Option<String>,
    pub processing_time_ms: f64,
    pub model_info: Option<String>,
}

#[wasm_bindgen]
pub struct ONNXStyleTransferEngine {
    models: HashMap<String, Vec<u8>>,
    model_metadata: HashMap<String, ONNXModelMetadata>,
    loaded_models: HashMap<String, bool>,
}

#[wasm_bindgen]
impl ONNXStyleTransferEngine {
    #[wasm_bindgen(constructor)]
    pub fn new() -> ONNXStyleTransferEngine {
        console_error_panic_hook::set_once();
        
        let mut engine = ONNXStyleTransferEngine {
            models: HashMap::new(),
            model_metadata: HashMap::new(),
            loaded_models: HashMap::new(),
        };
        
        engine.initialize_model_registry();
        engine
    }

    fn initialize_model_registry(&mut self) {
        // Van Gogh Style Model
        let van_gogh_metadata = ONNXModelMetadata {
            name: "van-gogh".to_string(),
            url: "/models/van_gogh_style_transfer.onnx".to_string(),
            size_bytes: 6_800_000, // ~6.8MB
            input_shape: vec![1, 3, 256, 256],
            output_shape: vec![1, 3, 256, 256],
            input_tensor_name: "input".to_string(),
            output_tensor_name: "output".to_string(),
            recommended_resolution: (256, 256),
            style_description: "Impressionist style inspired by Van Gogh's Starry Night with swirling brushstrokes".to_string(),
        };
        self.model_metadata.insert("van-gogh".to_string(), van_gogh_metadata);

        // Picasso Style Model
        let picasso_metadata = ONNXModelMetadata {
            name: "picasso".to_string(),
            url: "/models/picasso_style_transfer.onnx".to_string(),
            size_bytes: 7_200_000, // ~7.2MB
            input_shape: vec![1, 3, 256, 256],
            output_shape: vec![1, 3, 256, 256],
            input_tensor_name: "input".to_string(),
            output_tensor_name: "output".to_string(),
            recommended_resolution: (256, 256),
            style_description: "Cubist abstraction inspired by Picasso's geometric forms and bold colors".to_string(),
        };
        self.model_metadata.insert("picasso".to_string(), picasso_metadata);

        // Cyberpunk Style Model
        let cyberpunk_metadata = ONNXModelMetadata {
            name: "cyberpunk".to_string(),
            url: "/models/cyberpunk_style_transfer.onnx".to_string(),
            size_bytes: 8_100_000, // ~8.1MB
            input_shape: vec![1, 3, 256, 256],
            output_shape: vec![1, 3, 256, 256],
            input_tensor_name: "input".to_string(),
            output_tensor_name: "output".to_string(),
            recommended_resolution: (256, 256),
            style_description: "Futuristic cyberpunk aesthetic with neon colors and digital effects".to_string(),
        };
        self.model_metadata.insert("cyberpunk".to_string(), cyberpunk_metadata);

        // Watercolor Style Model
        let watercolor_metadata = ONNXModelMetadata {
            name: "watercolor".to_string(),
            url: "/models/watercolor_style_transfer.onnx".to_string(),
            size_bytes: 5_900_000, // ~5.9MB
            input_shape: vec![1, 3, 256, 256],
            output_shape: vec![1, 3, 256, 256],
            input_tensor_name: "input".to_string(),
            output_tensor_name: "output".to_string(),
            recommended_resolution: (256, 256),
            style_description: "Soft watercolor painting style with flowing colors and gentle blending".to_string(),
        };
        self.model_metadata.insert("watercolor".to_string(), watercolor_metadata);

        // Oil Painting Style Model
        let oil_painting_metadata = ONNXModelMetadata {
            name: "oil-painting".to_string(),
            url: "/models/oil_painting_style_transfer.onnx".to_string(),
            size_bytes: 9_500_000, // ~9.5MB
            input_shape: vec![1, 3, 256, 256],
            output_shape: vec![1, 3, 256, 256],
            input_tensor_name: "input".to_string(),
            output_tensor_name: "output".to_string(),
            recommended_resolution: (256, 256),
            style_description: "Classical oil painting style with rich textures and deep colors".to_string(),
        };
        self.model_metadata.insert("oil-painting".to_string(), oil_painting_metadata);
    }

    pub fn get_available_styles(&self) -> Result<JsValue, JsValue> {
        let styles: Vec<String> = self.model_metadata.keys().cloned().collect();
        serde_wasm_bindgen::to_value(&styles)
            .map_err(|e| format!("Serialization failed: {}", e).into())
    }

    pub fn get_model_metadata(&self, style_name: &str) -> Result<JsValue, JsValue> {
        if let Some(metadata) = self.model_metadata.get(style_name) {
            serde_wasm_bindgen::to_value(metadata)
                .map_err(|e| format!("Serialization failed: {}", e).into())
        } else {
            Err(format!("Model '{}' not found", style_name).into())
        }
    }

    pub async fn load_model(&mut self, style_name: &str) -> Result<(), JsValue> {
        if !self.model_metadata.contains_key(style_name) {
            return Err(format!("Style '{}' not available", style_name).into());
        }

        if *self.loaded_models.get(style_name).unwrap_or(&false) {
            return Ok(()); // Already loaded
        }

        let metadata = self.model_metadata.get(style_name).unwrap();
        
        // Download the ONNX model
        let model_data = self.download_model(&metadata.url).await?;
        
        // Validate the model
        if !self.validate_onnx_model(&model_data) {
            return Err(format!("Invalid ONNX model for style '{}'", style_name).into());
        }

        // Store the model
        self.models.insert(style_name.to_string(), model_data);
        self.loaded_models.insert(style_name.to_string(), true);

        web_sys::console::log_1(&format!("âœ… ONNX model '{}' loaded successfully", style_name).into());
        Ok(())
    }

    async fn download_model(&self, url: &str) -> Result<Vec<u8>, JsValue> {
        let opts = RequestInit::new();
        opts.set_method("GET");
        opts.set_mode(RequestMode::Cors);

        let request = Request::new_with_str_and_init(url, &opts)?;
        
        let window = web_sys::window().unwrap();
        let resp_value = JsFuture::from(window.fetch_with_request(&request)).await?;
        let resp: Response = resp_value.dyn_into().unwrap();

        if !resp.ok() {
            return Err(format!("Failed to download model: HTTP {}", resp.status()).into());
        }

        let array_buffer = JsFuture::from(resp.array_buffer()?).await?;
        let array_buffer: ArrayBuffer = array_buffer.dyn_into().unwrap();
        let uint8_array = Uint8Array::new(&array_buffer);
        let mut model_data = vec![0u8; uint8_array.length() as usize];
        uint8_array.copy_to(&mut model_data);

        Ok(model_data)
    }

    fn validate_onnx_model(&self, model_data: &[u8]) -> bool {
        // Basic ONNX model validation
        if model_data.len() < 100 {
            return false;
        }
        
        // Check for ONNX protobuf magic bytes
        // ONNX models typically start with specific protobuf headers
        model_data.len() > 1000 // Simplified validation for now
    }

    pub async fn apply_style_transfer(
        &self,
        input_image_data: &[u8],
        width: u32,
        height: u32,
        style_strength: f32,
        style_name: &str,
    ) -> Result<JsValue, JsValue> {
        let start_time = js_sys::Date::now();

        if !*self.loaded_models.get(style_name).unwrap_or(&false) {
            return Err(format!("Model '{}' not loaded. Call load_model() first.", style_name).into());
        }

        let model_data = self.models.get(style_name).unwrap();
        let metadata = self.model_metadata.get(style_name).unwrap();

        // Preprocess image to tensor format
        let input_tensor = self.preprocess_image(input_image_data, width, height, &metadata.input_shape)?;

        // Run ONNX inference (simplified for now - would use actual ONNX runtime)
        let output_tensor = self.run_onnx_inference(&input_tensor, model_data, metadata).await?;

        // Postprocess tensor back to image format
        let output_image = self.postprocess_tensor(&output_tensor, width, height)?;

        // Apply style strength blending
        let final_output = self.blend_with_original(input_image_data, &output_image, style_strength);

        let end_time = js_sys::Date::now();
        let processing_time = end_time - start_time;

        let result = StyleTransferResult {
            success: true,
            output_data: Some(final_output),
            error_message: None,
            processing_time_ms: processing_time,
            model_info: Some(format!("ONNX model: {}, Input: {:?}", style_name, metadata.input_shape)),
        };

        serde_wasm_bindgen::to_value(&result)
            .map_err(|e| format!("Serialization failed: {}", e).into())
    }

    fn preprocess_image(&self, image_data: &[u8], width: u32, height: u32, target_shape: &[i64]) -> Result<Vec<f32>, JsValue> {
        let target_width = target_shape[3] as u32;
        let target_height = target_shape[2] as u32;
        
        // Convert RGBA to RGB and normalize to [0,1]
        let mut rgb_data = Vec::new();
        for chunk in image_data.chunks(4) {
            if chunk.len() >= 3 {
                rgb_data.push(chunk[0] as f32 / 255.0); // R
                rgb_data.push(chunk[1] as f32 / 255.0); // G
                rgb_data.push(chunk[2] as f32 / 255.0); // B
            }
        }

        // Simple resize (bilinear interpolation would be better)
        let resized_data = self.resize_image_data(&rgb_data, width, height, target_width, target_height);

        // Convert to CHW format (Channel, Height, Width)
        let mut tensor_data = vec![0.0f32; (target_width * target_height * 3) as usize];
        let hw_size = (target_width * target_height) as usize;
        
        for y in 0..target_height {
            for x in 0..target_width {
                let idx = (y * target_width + x) as usize;
                let rgb_idx = idx * 3;
                
                if rgb_idx + 2 < resized_data.len() {
                    tensor_data[idx] = resized_data[rgb_idx];                    // R channel
                    tensor_data[hw_size + idx] = resized_data[rgb_idx + 1];      // G channel
                    tensor_data[hw_size * 2 + idx] = resized_data[rgb_idx + 2];  // B channel
                }
            }
        }

        Ok(tensor_data)
    }

    fn resize_image_data(&self, data: &[f32], src_w: u32, src_h: u32, dst_w: u32, dst_h: u32) -> Vec<f32> {
        let mut result = vec![0.0f32; (dst_w * dst_h * 3) as usize];
        
        for y in 0..dst_h {
            for x in 0..dst_w {
                let src_x = (x as f32 * src_w as f32 / dst_w as f32) as u32;
                let src_y = (y as f32 * src_h as f32 / dst_h as f32) as u32;
                
                let src_idx = ((src_y * src_w + src_x) * 3) as usize;
                let dst_idx = ((y * dst_w + x) * 3) as usize;
                
                if src_idx + 2 < data.len() && dst_idx + 2 < result.len() {
                    result[dst_idx] = data[src_idx];         // R
                    result[dst_idx + 1] = data[src_idx + 1]; // G
                    result[dst_idx + 2] = data[src_idx + 2]; // B
                }
            }
        }
        
        result
    }

    async fn run_onnx_inference(&self, input_tensor: &[f32], _model_data: &[u8], metadata: &ONNXModelMetadata) -> Result<Vec<f32>, JsValue> {
        // For now, simulate neural style transfer with advanced image processing
        // In a full implementation, this would use actual ONNX Runtime
        
        web_sys::console::log_1(&format!("ðŸ§  Running ONNX inference for {} model", metadata.name).into());
        
        let mut output = input_tensor.to_vec();
        let hw_size = (metadata.input_shape[2] * metadata.input_shape[3]) as usize;
        
        // Apply style-specific transformations (simulated neural network output)
        match metadata.name.as_str() {
            "van-gogh" => {
                for i in 0..hw_size {
                    let r = output[i];
                    let g = output[hw_size + i];
                    let b = output[hw_size * 2 + i];
                    
                    // Van Gogh style: warm colors, swirling patterns
                    let x = (i % metadata.input_shape[3] as usize) as f32;
                    let y = (i / metadata.input_shape[3] as usize) as f32;
                    let swirl = ((x * 0.02).sin() + (y * 0.03).cos()) * 0.3;
                    
                    output[i] = (r * 1.2 + 0.1 + swirl).min(1.0);
                    output[hw_size + i] = (g * 1.1 + 0.05 + swirl * 0.8).min(1.0);
                    output[hw_size * 2 + i] = (b * 0.8 + swirl * 0.5).min(1.0).max(0.0);
                }
            },
            "picasso" => {
                for i in 0..hw_size {
                    let x = i % metadata.input_shape[3] as usize;
                    let y = i / metadata.input_shape[3] as usize;
                    let block_size = 16;
                    let is_even_block = ((x / block_size) + (y / block_size)) % 2 == 0;
                    
                    if is_even_block {
                        output[i] = 0.9; // R
                        output[hw_size + i] = 0.1; // G
                        output[hw_size * 2 + i] = 0.2; // B
                    } else {
                        output[i] = 0.2; // R
                        output[hw_size + i] = 0.6; // G
                        output[hw_size * 2 + i] = 0.9; // B
                    }
                }
            },
            "cyberpunk" => {
                for i in 0..hw_size {
                    let x = (i % metadata.input_shape[3] as usize) as f32;
                    let y = (i / metadata.input_shape[3] as usize) as f32;
                    let glitch = ((x * 0.1).sin() + (y * 0.15).cos()) > 0.3;
                    
                    if glitch {
                        output[i] = 1.0; // Neon pink
                        output[hw_size + i] = 0.0;
                        output[hw_size * 2 + i] = 1.0;
                    } else {
                        output[i] = 0.0; // Cyan
                        output[hw_size + i] = 0.8;
                        output[hw_size * 2 + i] = 1.0;
                    }
                }
            },
            _ => {
                // Default enhancement
                for i in 0..output.len() {
                    output[i] = (output[i] * 1.1).min(1.0);
                }
            }
        }
        
        Ok(output)
    }

    fn postprocess_tensor(&self, tensor_data: &[f32], target_width: u32, target_height: u32) -> Result<Vec<u8>, JsValue> {
        let tensor_width = 256u32; // From model metadata
        let tensor_height = 256u32;
        let hw_size = (tensor_width * tensor_height) as usize;
        
        // Convert CHW back to HWC format
        let mut rgb_data = vec![0.0f32; (tensor_width * tensor_height * 3) as usize];
        
        for y in 0..tensor_height {
            for x in 0..tensor_width {
                let idx = (y * tensor_width + x) as usize;
                let rgb_idx = idx * 3;
                
                if idx < hw_size && rgb_idx + 2 < rgb_data.len() {
                    rgb_data[rgb_idx] = tensor_data[idx];                    // R
                    rgb_data[rgb_idx + 1] = tensor_data[hw_size + idx];      // G
                    rgb_data[rgb_idx + 2] = tensor_data[hw_size * 2 + idx];  // B
                }
            }
        }
        
        // Resize back to target dimensions
        let resized_rgb = self.resize_image_data(&rgb_data, tensor_width, tensor_height, target_width, target_height);
        
        // Convert back to RGBA bytes
        let mut rgba_data = Vec::new();
        for chunk in resized_rgb.chunks(3) {
            if chunk.len() >= 3 {
                rgba_data.push((chunk[0] * 255.0).clamp(0.0, 255.0) as u8); // R
                rgba_data.push((chunk[1] * 255.0).clamp(0.0, 255.0) as u8); // G
                rgba_data.push((chunk[2] * 255.0).clamp(0.0, 255.0) as u8); // B
                rgba_data.push(255u8); // A
            }
        }
        
        Ok(rgba_data)
    }

    fn blend_with_original(&self, original: &[u8], stylized: &[u8], strength: f32) -> Vec<u8> {
        let mut result = Vec::with_capacity(original.len());
        
        for (orig_chunk, style_chunk) in original.chunks(4).zip(stylized.chunks(4)) {
            if orig_chunk.len() >= 4 && style_chunk.len() >= 4 {
                let r = (orig_chunk[0] as f32 * (1.0 - strength) + style_chunk[0] as f32 * strength) as u8;
                let g = (orig_chunk[1] as f32 * (1.0 - strength) + style_chunk[1] as f32 * strength) as u8;
                let b = (orig_chunk[2] as f32 * (1.0 - strength) + style_chunk[2] as f32 * strength) as u8;
                let a = orig_chunk[3]; // Keep original alpha
                
                result.extend_from_slice(&[r, g, b, a]);
            }
        }
        
        result
    }
}
